Compiler Design i Compiler Design ii  About the Tutorial  i Audience  i Prerequisites  i Copyright & Disclaimer  i Table of Contents  ii 1. COMPILER DESIGN  OVERVIEW  1 Language Processing System  1 Preprocessor  2 Interpreter  2 Assembler  2 Linker  2 Loader  3 Cross-compiler  3 Source-to-source Compiler  3 2. COMPILER DESIGN ARCHITECTURE  4 Analysis Phase  4 Synthesis Phase 4 3. COMPILER DESIGN  PHASES OF COMPILER  5 Lexical Analysis  6 Syntax Analysis 6 Semantic Analysis  6 Intermediate Code Generation  6 Code Optimization  6 Code Generation  6 Symbol Table 7 4. COMPILER DESIGN  LEXICAL ANALYSIS  8 Tokens  8 Specifications of Tokens  9 Alphabets  9 Strings  9 Special Symbols  9 Language  10 5. COMPILER DESIGN  REGULAR EXPRESSIONS  11 Compiler Design iii Operations  11 Notations  11 Precedence and Associativity  12 6. COMPILER DESIGN  FINITE AUTOMATA  13 Finite Automata Construction  13 Longest Match Rule  14 7. COMPILER DESIGN  SYNTAX ANALYSIS  15 Context-Free Grammar  15 Syntax Analyzers  16 Derivation  17 Left-most Derivation  17 Right-most Derivation  17 Parse Tree  18 Ambiguity  21 Associativity  21 Precedence  22 Left Recursion  22 Left Factoring  24 First and Follow Sets  25 First Set  25 Follow Set  26 Limitations of Syntax Analyzers  26 8. COMPILER DESIGN  TYPES OF PARSING  27 Top-down Parsing  27 Bottom-up Parsing 27 9. COMPILER DESIGN  TOP-DOWN PARSING  29 Recursive Descent Parsing  29 Back-tracking  30 Predictive Parser  30 LL Parser  32 LL Parsing Algorithm  32 10. COMPILER DESIGN  BOTTOM-UP PARSING  34 Shift-Reduce Parsing  34 Compiler Design iv LR Parser  34 LL vs. LR  36 11. COMPILER DESIGN  ERROR RECOVERY  37 Panic Mode  37 Statement Mode  37 Error Productions  37 Global Correction  37 Abstract Syntax Trees  38 12. COMPILER DESIGN  SEMANTIC ANALYSIS  40 Semantics  40 Semantic Errors  41 Attribute Grammar  41 Synthesized Attributes  41 Inherited Attributes  42 S-attributed SDT  43 L-attributed SDT  43 13. COMPILER DESIGN  RUNTIME ENVIRONMENT  45 Activation Trees 45 Storage Allocation  47 Static Allocation  47 Stack Allocation  48 Heap Allocation  48 Parameter Passing  49 r-value  49 l-value  49 Formal Parameters  49 Actual Parameters  50 Pass by Value  50 Pass by Reference  50 Pass by Copy-restore  50 Compiler Design v Pass by Name  51 14. COMPILER DESIGN  SYMBOL TABLE  52 Implementation 52 Operations  53 insert()  53 lookup()  53 Scope Management  54 15. COMPILER DESIGN   INTERMEDIATE CODE GENERATION  56 Intermediate Representation  56 Three-Address Code  57 Declarations  58 16. COMPILER DESIGN  CODE GENERATION  60 Directed Acyclic Graph  60 Peephole Optimization  61 Redundant Instruction Elimination  61 Unreachable Code  62 Flow of Control Optimization 62 Algebraic Expression Simplification  63 Strength Reduction  63 Accessing Machine Instructions  63 Code Generator  63 Descriptors  64 Code Generation  64 17. COMPILER DESIGN  CODE OPTIMIZATION  66 Machine-independent Optimization  66 Machine-dependent Optimization  67 Basic Blocks  67 Basic Block Identification  67 Control Flow Graph  68 Loop Optimization  69 Compiler Design vi Dead-code Elimination  69 Partially Dead Code 70 Partial Redundancy  71   Compiler Design 7 Computers are a balanced mix of software and hardware. Hardware is just a piece of mechanical device and its functions are being controlled by a compatible software. Hardware understands instructions in the form of electronic charge, which is the counterpart of binary language in software programming. Binary language has only two alphabets, 0 and 1. To instruct, the hardware codes must be written in binary format, which is simply a series of 1s and 0s. It would be a difficult and cumbersome task for computer programmers to write such codes, which is why we have compilers to write such codes.  We have learnt that any computer system is made of hardware and software. The hardware understands a language, which humans cannot understand. So we write programs in high-level language, which is easier for us to understand and remember. These programs are then fed into a series of tools and OS components to get the desired code that can be used by the machine. This is known as Language Processing System.  1. COMPILER DESIGN  OVERVIEW Compiler Design 8 The high-level language is converted into binary language in various phases. A compiler is a program that converts high-level language to assembly language. Similarly, an assembler is a program that converts the assembly language to machine-level language. Let us first understand how a program, using C compiler, is executed on a host machine.  User writes a program in C language (high-level language).  The C compiler compiles the program and translates it to assembly program (low-level language).  An assembler then translates the assembly program into machine code (object).  A linker tool is used to link all the parts of the program together for execution (executable machine code).  A loader loads all of them into memory and then the program is executed. Before diving straight into the concepts of compilers, we should understand a few other tools that work closely with compilers.  A preprocessor, generally considered as a part of compiler, is a tool that produces input for compilers. It deals with macro-processing, augmentation, file inclusion, language extension, etc.  An interpreter, like a compiler, translates high-level language into low-level machine language. The difference lies in the way they read the source code or input. A compiler reads the whole source code at once, creates tokens, checks semantics, generates intermediate code, executes the whole program and may involve many passes. In contrast, an interpreter reads a statement from the input, converts it to an intermediate code, executes it, then takes the next statement in sequence. If an error occurs, an interpreter stops execution and reports it; whereas a compiler reads the whole program even if it encounters several errors.  An assembler translates assembly language programs into machine code. The output of an assembler is called an object file, which contains a combination of machine instructions as well as the data required to place these instructions in memory.  Linker is a computer program that links and merges various object files together in order to make an executable file. All these files might have been compiled by separate assemblers. The major task of a linker is to search and locate referenced module/routines in a program and to determine the memory location where these codes will be loaded, making the program instruction to have absolute references. Compiler Design 9  Loader is a part of operating system and is responsible for loading executable files into memory and execute them. It calculates the size of a program (instructions and data) and creates memory space for it. It initializes various registers to initiate execution.  A compiler that runs on platform (A) and is capable of generating executable code for platform (B) is called a cross-compiler.  A compiler that takes the source code of one programming language and translates it into the source code of another programming language is called a source-to-source compiler. Compiler Design 10 A compiler can broadly be divided into two phases based on the way they compile.  Known as the front-end of the compiler, the analysis phase of the compiler reads the source program, divides it into core parts, and then checks for lexical, grammar, and syntax errors. The analysis phase generates an intermediate representation of the source program and symbol table, which should be fed to the Synthesis phase as input.   Known as the back-end of the compiler, the synthesis phase generates the target program with the help of intermediate source code representation and symbol table. A compiler can have many phases and passes.  Pass : A pass refers to the traversal of a compiler through the entire program.  Phase : A phase of a compiler is a distinguishable stage, which takes input from the previous stage, processes and yields output that can be used as input for the next stage. A pass can have more than one phase. 2. COMPILER DESIGN ARCHITECTURE Compiler Design 11 The compilation process is a sequence of various phases. Each phase takes input from its previous stage, has its own representation of source program, and feeds its output to the next phase of the compiler. Let us understand the phases of a compiler.  3. COMPILER DESIGN  PHASES OF COMPILER Compiler Design 12  The first phase of scanner works as a text scanner. This phase scans the source code as a stream of characters and converts it into meaningful lexemes. Lexical analyzer represents these lexemes in the form of tokens as: <token-name, attribute-value>  The next phase is called the syntax analysis or parsing. It takes the token produced by lexical analysis as input and generates a parse tree (or syntax tree). In this phase, token arrangements are checked against the source code grammar, i.e., the parser checks if the expression made by the tokens is syntactically correct.  Semantic analysis checks whether the parse tree constructed follows the rules of language. For example, assignment of values is between compatible data types, and adding string to an integer. Also, the semantic analyzer keeps track of identifiers, their types and expressions; whether identifiers are declared before use or not, etc. The semantic analyzer produces an annotated syntax tree as an output.  After semantic analysis, the compiler generates an intermediate code of the source code for the target machine. It represents a program for some abstract machine. It is in between the high-level language and the machine language. This intermediate code should be generated in such a way that it makes it easier to be translated into the target machine code.  The next phase does code optimization of the intermediate code. Optimization can be assumed as something that removes unnecessary code lines, and arranges the sequence of statements in order to speed up the program execution without wasting resources (CPU, memory).  In this phase, the code generator takes the optimized representation of the intermediate code and maps it to the target machine language. The code generator translates the intermediate code into a sequence of (generally) re-locatable machine code. Sequence of instructions of machine code performs the task as the intermediate code would do. Compiler Design 13  It is a data-structure maintained throughout all the phases of a compiler. All the identifiers names along with their types are stored here. The symbol table makes it easier for the compiler to quickly search the identifier record and retrieve it. The symbol table is also used for scope management.  Compiler Design 14 Lexical analysis is the first phase of a compiler. It takes the modified source code from language preprocessors that are written in the form of sentences. The lexical analyzer breaks these syntaxes into a series of tokens, by removing any whitespace or comments in the source code. If the lexical analyzer finds a token invalid, it generates an error. The lexical analyzer works closely with the syntax analyzer. It reads character streams from the source code, checks for legal tokens, and passes the data to the syntax analyzer when it demands.   Lexemes are said to be a sequence of characters (alphanumeric) in a token. There are some predefined rules for every lexeme to be identified as a valid token. These rules are defined by grammar rules, by means of a pattern. A pattern explains what can be a token, and these patterns are defined by means of regular expressions. In programming language, keywords, constants, identifiers, strings, numbers, operators, and punctuations symbols can be considered as tokens. For example, in C language, the variable declaration line int value = 100; contains the tokens: int (keyword), value (identifier), = (operator), 100 (constant) and ; (symbol).  4. COMPILER DESIGN  LEXICAL ANALYSIS Compiler Design 15  Let us understand how the language theory undertakes the following terms:  Any finite set of symbols {0,1} is a set of binary alphabets, {0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F} is a set of Hexadecimal alphabets, {a-z, A-Z} is a set of English language alphabets.  Any finite sequence of alphabets is called a string. Length of the string is the total number of occurrence of alphabets, e.g., the length of the string tutorialspoint is 14 and is denoted by |tutorialspoint| = 14. A string having no alphabets, i.e. a string of zero length is known as an   A typical high-level language contains the following symbols:-  Arithmetic Symbols Addition(+), Subtraction(-), Modulo(%), Multiplication(*), Division(/) Punctuation Comma(,), Semicolon(;), Dot(.), Arrow(->) Assignment = Special Assignment +=, /=, *=, -= Comparison ==, !=, <, <=, >, >= Preprocessor # Compiler Design 16 Location Specifier & Logical &, &&, |, ||, ! Shift Operator >>, >>>, <<, <<<  A language is considered as a finite set of strings over some finite set of alphabets. Computer languages are considered as finite sets, and mathematically set operations can be performed on them. Finite languages can be described by means of regular expressions. Compiler Design 17 The lexical analyzer needs to scan and identify only a finite set of valid string/token/lexeme that belong to the language in hand. It searches for the pattern defined by the language rules. Regular expressions have the capability to express finite languages by defining a pattern for finite strings of symbols. The grammar defined by regular expressions is known as regular grammar. The language defined by regular grammar is known as regular language. Regular expression is an important notation for specifying patterns. Each pattern matches a set of strings, so regular expressions serve as names for a set of strings. Programming language tokens can be described by regular languages. The specification of regular expressions is an example of a recursive definition. Regular languages are easy to understand and have efficient implementation. There are a number of algebraic laws that are obeyed by regular expressions, which can be used to manipulate regular expressions into equivalent forms.  The various operations on languages are:  Union of two languages L and M is written as L U M = {s | s is in L or s is in M}  Concatenation of two languages L and M is written as LM = {st | s is in L and t is in M}  The Kleene Closure of a language L is written as L* = Zero or more occurrence of language L.  If r and s are regular expressions denoting the languages L(r) and L(s), then  Union : (r)|(s) is a regular expression denoting L(r) U L(s)  Concatenation : (r)(s) is a regular expression denoting L(r)L(s)  Kleene closure : (r)* is a regular expression denoting (L(r))*  (r) is a regular expression denoting L(r)   *, concatenation (.), and | (pipe sign) are left associative  * has the highest precedence 5. COMPILER DESIGN  REGULAR EXPRESSIONS Compiler Design 18  Concatenation (.) has the second highest precedence.  | (pipe sign) has the lowest precedence of all. Representing valid tokens of a language in regular expression If x is a regular expression, then:  x* means zero or more occurrence of x.   x+ means one or more occurrence of x.   x? means at most one occurrence of x i.e., it can generate either {x} or {e}. [a-z] is all lower-case alphabets of English language. [A-Z] is all upper-case alphabets of English language. [0-9] is all natural digits used in mathematics. Representing occurrence of symbols using regular expressions letter = [a  z] or [A  Z] digit = 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 or [0-9] sign = [ + | - ] Representing language tokens using regular expressions Decimal = (sign)?(digit)+ Identifier = (letter)(letter | digit)* The only problem left with the lexical analyzer is how to verify the validity of a regular expression used in specifying the patterns of keywords of a language. A well-accepted solution is to use finite automata for verification. Compiler Design 19 Finite automata is a state machine that takes a string of symbols as input and changes its state accordingly. Finite automata is a recognizer for regular expressions. When a regular expression string is fed into finite automata, it changes its state for each literal. If the input string is successfully processed and the automata reaches its final state, it is accepted, i.e., the string just fed was said to be a valid token of the language in hand. The mathematical model of finite automata consists of:  Finite set of states (Q)    One Start state (q0)  Set of final states (qf)    Q  Let L(r) be a regular language recognized by some finite automata (FA).  States : States of FA are represented by circles. State names are written inside circles.  Start state : The state from where the automata starts is known as the start state. Start state has an arrow pointed towards it.  Intermediate states : All intermediate states have at least two arrows; one pointing to and another pointing out from them.  Final state : If the input string is successfully parsed, the automata is expected to be in this state. Final state is represented by double circles. It may have any odd number of arrows pointing to it and even number of arrows pointing out from it. The number of odd arrows are one greater than even, i.e. odd = even+1.  Transition : The transition from one state to another state happens when a desired symbol in the input is found. Upon transition, automata can either move to the next state or stay in the same state. Movement from one state to another is shown as a directed arrow, where the arrows point to the destination state. If automata stays on the same state, an arrow pointing from a state to itself is drawn. Example : We assume FA accepts any three digit binary value ending in digit 1. FA = {Q(q0, qf0, qf 6. COMPILER DESIGN  FINITE AUTOMATA Compiler Design 20   When the lexical analyzer read the source-code, it scans the code letter by letter; and when it encounters a whitespace, operator symbol, or special symbols, it decides that a word is completed. For example: int intvalue; yzer cannot determine whether it is a keyword int or the initials of identifier int value. The Longest Match Rule states that the lexeme scanned should be determined based on the longest match among all the tokens available. The lexical analyzer also follows rule priority where a reserved word, e.g., a keyword, of a language is given priority over user input. That is, if the lexical analyzer finds a lexeme that matches with any existing reserved word, it should generate an error.   